from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, avg, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
import os

from configs import kafka_config

jdbc_url = "jdbc:mysql://217.61.57.46:3306/olympic_dataset"
jdbc_user = "neo_data_admin"
jdbc_password = "Proyahaxuqithab9oplp"

input_topic = "athlete_event_results"
output_topic = "athlete_stats"
jdbc_jar = "mysql-connector-j-8.0.32.jar"

if not os.path.exists(jdbc_jar):
    print(f"–£–≤–∞–≥–∞: JDBC –¥—Ä–∞–π–≤–µ—Ä {jdbc_jar} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ –≤—ñ–Ω –¥–æ—Å—Ç—É–ø–Ω–∏–π.")

print("–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—é Spark —Å–µ—Å—ñ—é...")
spark = SparkSession.builder \
    .appName("AthleteBioAndResultsStreamingPipeline") \
    .master("local[*]") \
    .config("spark.jars", jdbc_jar) \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .config("spark.driver.host", "127.0.0.1") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# –ï—Ç–∞–ø 1: –ó—á–∏—Ç–∞—Ç–∏ –¥–∞–Ω—ñ —Ñ—ñ–∑–∏—á–Ω–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤ –∞—Ç–ª–µ—Ç—ñ–≤ –∑ MySQL
print("–ï—Ç–∞–ø 1: –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö —Ñ—ñ–∑–∏—á–Ω–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤ –∞—Ç–ª–µ—Ç—ñ–≤ –∑ MySQL")
athlete_bio_df = spark.read.format('jdbc').options(
    url=jdbc_url,
    driver='com.mysql.cj.jdbc.Driver',
    dbtable="athlete_bio",
    user=jdbc_user,
    password=jdbc_password).load()

athlete_bio_df.printSchema()
athlete_bio_df.show(5)

# –ï—Ç–∞–ø 2: –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è
print("–ï—Ç–∞–ø 2: –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –∑–∞ –∑—Ä–æ—Å—Ç–æ–º —Ç–∞ –≤–∞–≥–æ—é")
filtered_bio_df = athlete_bio_df.filter(
    col("height").isNotNull() &
    col("weight").isNotNull() &
    col("height").cast("float").isNotNull() &
    col("weight").cast("float").isNotNull()
)
print("Filtered Bio Data Count:", filtered_bio_df.count())
filtered_bio_df.show(5)

# –ï—Ç–∞–ø 3a: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ MySQL —Ç–∞ –∑–∞–ø–∏—Å —É Kafka
print("–ï—Ç–∞–ø 3a: –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑–º–∞–≥–∞–Ω—å –∑ MySQL")
event_results_df = spark.read.format('jdbc').options(
    url=jdbc_url,
    driver='com.mysql.cj.jdbc.Driver',
    dbtable="athlete_event_results",
    user=jdbc_user,
    password=jdbc_password).load()

event_results_df.printSchema()
event_results_df.show(5)

print("–ó–∞–ø–∏—Å —É Kafka...")
event_results_df.selectExpr("to_json(struct(*)) AS value") \
    .write \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_config['bootstrap_servers'][0]) \
    .option("kafka.security.protocol", kafka_config['security_protocol']) \
    .option("kafka.sasl.mechanism", kafka_config['sasl_mechanism']) \
    .option("kafka.sasl.jaas.config",
            f'org.apache.kafka.common.security.plain.PlainLoginModule required username="{kafka_config["username"]}" password="{kafka_config["password"]}";') \
    .option("topic", input_topic) \
    .save()

print("Data has been written to Kafka topic.")

# –°—Ö–µ–º–∞ –¥–ª—è Kafka
event_schema = StructType([
    StructField("edition", StringType(), True),
    StructField("edition_id", IntegerType(), True),
    StructField("country_noc", StringType(), True),
    StructField("sport", StringType(), True),
    StructField("event", StringType(), True),
    StructField("result_id", IntegerType(), True),
    StructField("athlete", StringType(), True),
    StructField("athlete_id", IntegerType(), True),
    StructField("pos", StringType(), True),
    StructField("medal", StringType(), True),
    StructField("isTeamSport", StringType(), True)
])

# –ï—Ç–∞–ø 3b: –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –∑ Kafka
print("–ï—Ç–∞–ø 3b: –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Kafka stream")
kafka_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_config['bootstrap_servers'][0]) \
    .option("kafka.security.protocol", kafka_config['security_protocol']) \
    .option("kafka.sasl.mechanism", kafka_config['sasl_mechanism']) \
    .option("kafka.sasl.jaas.config",
            f'org.apache.kafka.common.security.plain.PlainLoginModule required username="{kafka_config["username"]}" password="{kafka_config["password"]}";') \
    .option("subscribe", input_topic) \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 20) \
    .load()

parsed_stream = kafka_stream \
    .selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), event_schema).alias("data")) \
    .select("data.*") \
    .withColumn("athlete_id", col("athlete_id").cast("int"))

# –ï—Ç–∞–ø 4-6: –û–±—Ä–æ–±–∫–∞ –±–∞—Ç—á—ñ–≤
def process_batch(batch_df, batch_id):
    print(f"\n====================")
    print(f"‚öôÔ∏è –û–±—Ä–æ–±–∫–∞ –±–∞—Ç—á—É {batch_id}")

    try:
        if batch_df.rdd.isEmpty():
            print(f"‚ö†Ô∏è –ë–∞—Ç—á {batch_id} –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Äî –Ω—ñ—á–æ–≥–æ –Ω–µ –æ–±—Ä–æ–±–ª—è—î–º–æ.")
            return

        print(f"‚úÖ Batch {batch_id} –º—ñ—Å—Ç–∏—Ç—å {batch_df.count()} —Ä—è–¥–∫—ñ–≤")
        print("üîç –ü–µ—Ä—à—ñ 5 —Ä—è–¥–∫—ñ–≤ batch_df:")
        print(batch_df.limit(5).collect())  # –û–±–µ—Ä–µ–∂–Ω–æ –∑ –≤–µ–ª–∏–∫–∏–º–∏ –æ–± º—î–º–∞–º–∏

        print("üß© –°—Ö–µ–º–∞ batch_df:")
        batch_df.printSchema()

        # –ï—Ç–∞–ø 4: –û–± º—î–¥–Ω–∞–Ω–Ω—è
        print("üîó –ï—Ç–∞–ø 4: –û–± º—î–¥–Ω–∞–Ω–Ω—è –∑ athlete_bio")
        joined_df = batch_df.join(
            filtered_bio_df,
            batch_df.athlete_id == filtered_bio_df.athlete_id,
            "inner"
        )

        print(f"üîé –û–± º—î–¥–Ω–∞–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤: {joined_df.count()}")
        joined_df.show(5)

        # –ï—Ç–∞–ø 5: –û–±—á–∏—Å–ª–µ–Ω–Ω—è
        print("üìä –ï—Ç–∞–ø 5: –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
        stats_df = joined_df.groupBy(
            "sport",
            "medal",
            filtered_bio_df["sex"],
            filtered_bio_df["country_noc"]
        ).agg(
            avg("height").alias("avg_height"),
            avg("weight").alias("avg_weight")
        ).withColumn("timestamp", current_timestamp())

        print("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:")
        stats_df.show()

        # –ï—Ç–∞–ø 6a: –ó–∞–ø–∏—Å —É Kafka
        print("üì§ –ï—Ç–∞–ø 6a: –ó–∞–ø–∏—Å —É Kafka")
        stats_df.selectExpr("to_json(struct(*)) AS value") \
            .write \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_config['bootstrap_servers'][0]) \
            .option("kafka.security.protocol", kafka_config['security_protocol']) \
            .option("kafka.sasl.mechanism", kafka_config['sasl_mechanism']) \
            .option("kafka.sasl.jaas.config",
                    f'org.apache.kafka.common.security.plain.PlainLoginModule required username="{kafka_config["username"]}" password="{kafka_config["password"]}";') \
            .option("topic", output_topic) \
            .save()

        # –ï—Ç–∞–ø 6b: –ó–∞–ø–∏—Å —É MySQL
        print("üíæ –ï—Ç–∞–ø 6b: –ó–∞–ø–∏—Å —É MySQL")
        stats_df.write \
            .format("jdbc") \
            .option("url", jdbc_url) \
            .option("driver", "com.mysql.cj.jdbc.Driver") \
            .option("dbtable", "sergijr.athlete_stats") \
            .option("user", jdbc_user) \
            .option("password", jdbc_password) \
            .mode("append") \
            .save()

        print(f"‚úÖ –ë–∞—Ç—á {batch_id} —É—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ.")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –±–∞—Ç—á—É {batch_id}: {str(e)}")

# –ó–∞–ø—É—Å–∫ —Å—Ç—Ä–∏–º—ñ–Ω–≥—É
print("–ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫—É...")
query = parsed_stream \
    .writeStream \
    .foreachBatch(process_batch) \
    .outputMode("update") \
    .trigger(processingTime="10 seconds") \
    .start()

print("‚úÖ –ü–æ—Ç–æ–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç –∑–∞–ø—É—â–µ–Ω–æ. –û—á—ñ–∫—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è...")
query.awaitTermination()